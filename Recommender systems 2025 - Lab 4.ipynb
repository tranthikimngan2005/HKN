{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae044c1",
   "metadata": {},
   "source": [
    "# Hệ khuyến nghị. IUH 2025.\n",
    "### Ngày 18/9/2025. Lab 4.\n",
    "Mục tiêu: ôn tập SVD và content-based (bằng 2 cách)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f962cc7-a444-4fcb-99db-afe0aef0096d",
   "metadata": {},
   "source": [
    "**Bài 1.** \n",
    "\n",
    "The SVD can be calculated by calling the svd() function. The function takes a matrix and returns the U, Sigma and V^T elements. The Sigma diagonal matrix is returned as a vector of singular values. The V matrix is returned in a transposed form, e.g. V.T.\n",
    "\n",
    "The example below defines a $3 \\times 2$ matrix and calculates the Singular-value decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6edb278b-a3bc-4438-9a18-779729a1cbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[-0.2298477   0.88346102  0.40824829]\n",
      " [-0.52474482  0.24078249 -0.81649658]\n",
      " [-0.81964194 -0.40189603  0.40824829]]\n",
      "[9.52551809 0.51430058]\n",
      "[[-0.61962948 -0.78489445]\n",
      " [-0.78489445  0.61962948]]\n"
     ]
    }
   ],
   "source": [
    "# Singular-value decomposition\n",
    "from numpy import array\n",
    "from scipy.linalg import svd\n",
    "# define a matrix\n",
    "A = array([[1, 2], [3, 4], [5, 6]])\n",
    "print(A)\n",
    "# SVD\n",
    "U, s, VT = svd(A)\n",
    "print(U)\n",
    "print(s)\n",
    "print(VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9e5ae1-6417-4c5e-8a12-f6ddf90c2ab3",
   "metadata": {},
   "source": [
    "**Reconstruct Matrix from SVD:** The original matrix can be reconstructed from the U, Sigma, and V^T elements.\n",
    "\n",
    "The U, s, and V elements returned from the svd() cannot be multiplied directly. \n",
    "The s vector must be converted into a diagonal matrix using the diag() function. By default, this function will create a square matrix that is n x n, relative to our original matrix. This causes a problem as the size of the matrices do not fit the rules of matrix multiplication, where the number of columns in a matrix must match the number of rows in the subsequent matrix.\n",
    "\n",
    "After creating the square Sigma diagonal matrix, the sizes of the matrices are relative to the original m x n matrix that we are decomposing, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "73e82f1d-d5e0-41ae-ac4e-7623a7b16191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct SVD\n",
    "from numpy import array\n",
    "from numpy import diag\n",
    "from numpy import dot\n",
    "from numpy import zeros\n",
    "from scipy.linalg import svd\n",
    "# define a matrix\n",
    "A = array([[1, 2], [3, 4], [5, 6]])\n",
    "print(A)\n",
    "# Singular-value decomposition\n",
    "U, s, VT = svd(A)\n",
    "# create m x n Sigma matrix\n",
    "Sigma = zeros((A.shape[0], A.shape[1]))\n",
    "# populate Sigma with n x n diagonal matrix\n",
    "Sigma[:A.shape[1], :A.shape[1]] = diag(s)\n",
    "# reconstruct matrix\n",
    "B = U.dot(Sigma.dot(VT))\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d148e-34d6-42c4-9cc4-04f956277e62",
   "metadata": {},
   "source": [
    "SV tham khảo thêm ứng dụng về topic modeling sử dụng SVD tại đây:\n",
    "\n",
    "https://www.freecodecamp.org/news/advanced-topic-modeling-how-to-use-svd-nmf-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e774c5-3dc6-4d8f-8dc6-e8e25a70a36d",
   "metadata": {},
   "source": [
    "**Bài 2.** Thao tác content-based với CSDL **movie-len 100k**. SV download tại đây (giải nén và đặt cùng địa chỉ file notebook):\n",
    "https://grouplens.org/datasets/movielens/100k/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74ad4b78-841b-47e2-8f69-101b4a80dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse \n",
    "\n",
    "class MF(object):\n",
    "    \"\"\"docstring for CF\"\"\"\n",
    "    def __init__(self, Y_data, K, lam = 0.1, Xinit = None, Winit = None, \n",
    "            learning_rate = 0.5, max_iter = 1000, print_every = 100, user_based = 1):\n",
    "        self.Y_raw_data = Y_data\n",
    "        self.K = K\n",
    "        # regularization parameter\n",
    "        self.lam = lam\n",
    "        # learning rate for gradient descent\n",
    "        self.learning_rate = learning_rate\n",
    "        # maximum number of iterations\n",
    "        self.max_iter = max_iter\n",
    "        # print results after print_every iterations\n",
    "        self.print_every = print_every\n",
    "        # user-based or item-based\n",
    "        self.user_based = user_based\n",
    "        # number of users, items, and ratings. Remember to add 1 since id starts from 0\n",
    "        self.n_users = int(np.max(Y_data[:, 0])) + 1 \n",
    "        self.n_items = int(np.max(Y_data[:, 1])) + 1\n",
    "        self.n_ratings = Y_data.shape[0]\n",
    "        \n",
    "        if Xinit is None: # new\n",
    "            self.X = np.random.randn(self.n_items, K)\n",
    "        else: # or from saved data\n",
    "            self.X = Xinit \n",
    "        \n",
    "        if Winit is None: \n",
    "            self.W = np.random.randn(K, self.n_users)\n",
    "        else: # from daved data\n",
    "            self.W = Winit\n",
    "            \n",
    "        # normalized data, update later in normalized_Y function\n",
    "        self.Y_data_n = self.Y_raw_data.copy()\n",
    "\n",
    "\n",
    "    def normalize_Y(self):\n",
    "        if self.user_based:\n",
    "            user_col = 0\n",
    "            item_col = 1\n",
    "            n_objects = self.n_users\n",
    "\n",
    "        # if we want to normalize based on item, just switch first two columns of data\n",
    "        else: # item bas\n",
    "            user_col = 1\n",
    "            item_col = 0 \n",
    "            n_objects = self.n_items\n",
    "\n",
    "        users = self.Y_raw_data[:, user_col] \n",
    "        self.mu = np.zeros((n_objects,))\n",
    "        for n in range(n_objects):\n",
    "            # row indices of rating done by user n\n",
    "            # since indices need to be integers, we need to convert\n",
    "            ids = np.where(users == n)[0].astype(np.int32)\n",
    "            # indices of all ratings associated with user n\n",
    "            item_ids = self.Y_data_n[ids, item_col] \n",
    "            # and the corresponding ratings \n",
    "            ratings = self.Y_data_n[ids, 2]\n",
    "            # take mean\n",
    "            m = np.mean(ratings) \n",
    "            if np.isnan(m):\n",
    "                m = 0 # to avoid empty array and nan value\n",
    "            self.mu[n] = m\n",
    "            # normalize\n",
    "            self.Y_data_n[ids, 2] = ratings - self.mu[n]\n",
    "    \n",
    "    # Tính giá trị hàm mất mát:\n",
    "    \n",
    "    def loss(self):\n",
    "        L = 0 \n",
    "        for i in range(self.n_ratings):\n",
    "            # user, item, rating\n",
    "            n, m, rate = int(self.Y_data_n[i, 0]), int(self.Y_data_n[i, 1]), self.Y_data_n[i, 2]\n",
    "            L += 0.5*(rate - self.X[m, :].dot(self.W[:, n]))**2\n",
    "        \n",
    "        # take average\n",
    "        L /= self.n_ratings\n",
    "        # regularization, don't ever forget this \n",
    "        L += 0.5*self.lam*(np.linalg.norm(self.X, 'fro') + np.linalg.norm(self.W, 'fro'))\n",
    "        return L \n",
    "\n",
    "    # Xác định các items được đánh giá bởi 1 user, và users đã đánh giá 1 item và các ratings tương ứng:\n",
    "    \n",
    "    def get_items_rated_by_user(self, user_id):\n",
    "        \"\"\"\n",
    "        get all items which are rated by user user_id, and the corresponding ratings\n",
    "        \"\"\"\n",
    "        ids = np.where(self.Y_data_n[:,0] == user_id)[0] \n",
    "        item_ids = self.Y_data_n[ids, 1].astype(np.int32) # indices need to be integers\n",
    "        ratings = self.Y_data_n[ids, 2]\n",
    "        return (item_ids, ratings)\n",
    "        \n",
    "        \n",
    "    def get_users_who_rate_item(self, item_id):\n",
    "        \"\"\"\n",
    "        get all users who rated item item_id and get the corresponding ratings\n",
    "        \"\"\"\n",
    "        ids = np.where(self.Y_data_n[:,1] == item_id)[0] \n",
    "        user_ids = self.Y_data_n[ids, 0].astype(np.int32)\n",
    "        ratings = self.Y_data_n[ids, 2]\n",
    "        return (user_ids, ratings)\n",
    "\n",
    "    # Cập nhật X, W:\n",
    "\n",
    "    def updateX(self):\n",
    "        for m in range(self.n_items):\n",
    "            user_ids, ratings = self.get_users_who_rate_item(m)\n",
    "            Wm = self.W[:, user_ids]\n",
    "            # gradient\n",
    "            grad_xm = -(ratings - self.X[m, :].dot(Wm)).dot(Wm.T)/self.n_ratings + \\\n",
    "                                               self.lam*self.X[m, :]\n",
    "            self.X[m, :] -= self.learning_rate*grad_xm.reshape((self.K,))\n",
    "    \n",
    "    def updateW(self):\n",
    "        for n in range(self.n_users):\n",
    "            item_ids, ratings = self.get_items_rated_by_user(n)\n",
    "            Xn = self.X[item_ids, :]\n",
    "            # gradient\n",
    "            grad_wn = -Xn.T.dot(ratings - Xn.dot(self.W[:, n]))/self.n_ratings + \\\n",
    "                        self.lam*self.W[:, n]\n",
    "            self.W[:, n] -= self.learning_rate*grad_wn.reshape((self.K,))\n",
    "\n",
    "    # Phần thuật toán chính:\n",
    "    \n",
    "    def fit(self):\n",
    "        self.normalize_Y()\n",
    "        for it in range(self.max_iter):\n",
    "            self.updateX()\n",
    "            self.updateW()\n",
    "            if (it + 1) % self.print_every == 0:\n",
    "                rmse_train = self.evaluate_RMSE(self.Y_raw_data)\n",
    "                print ('iter =', it + 1, ', loss =', self.loss(), ', RMSE train =', rmse_train)\n",
    "\n",
    "    # Dự đoán:\n",
    "    \n",
    "    def pred(self, u, i):\n",
    "        \"\"\" \n",
    "        predict the rating of user u for item i \n",
    "        if you need the un\n",
    "        \"\"\"\n",
    "        u = int(u)\n",
    "        i = int(i)\n",
    "        if self.user_based:\n",
    "            bias = self.mu[u]\n",
    "        else: \n",
    "            bias = self.mu[i]\n",
    "        pred = self.X[i, :].dot(self.W[:, u]) + bias \n",
    "        # truncate if results are out of range [0, 5]\n",
    "        if pred < 0:\n",
    "            return 0 \n",
    "        if pred > 5: \n",
    "            return 5 \n",
    "        return pred \n",
    "        \n",
    "    \n",
    "    def pred_for_user(self, user_id):\n",
    "        \"\"\"\n",
    "        predict ratings one user give all unrated items\n",
    "        \"\"\"\n",
    "        ids = np.where(self.Y_data_n[:, 0] == user_id)[0]\n",
    "        items_rated_by_u = self.Y_data_n[ids, 1].tolist()              \n",
    "        \n",
    "        y_pred = self.X.dot(self.W[:, user_id]) + self.mu[user_id]\n",
    "        predicted_ratings= []\n",
    "        for i in range(self.n_items):\n",
    "            if i not in items_rated_by_u:\n",
    "                predicted_ratings.append((i, y_pred[i]))\n",
    "        return predicted_ratings\n",
    "\n",
    "    # Đánh giá kết quả bằng cách đo Root Mean Square Error:\n",
    "    \n",
    "    def evaluate_RMSE(self, rate_test):\n",
    "        n_tests = rate_test.shape[0]\n",
    "        SE = 0 # squared error\n",
    "        for n in range(n_tests):\n",
    "            pred = self.pred(rate_test[n, 0], rate_test[n, 1])\n",
    "            SE += (pred - rate_test[n, 2])**2 \n",
    "\n",
    "        RMSE = np.sqrt(SE/n_tests)\n",
    "        return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b715aac3-8d03-4225-8b1e-612ddc347dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading ratings file:\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "\n",
    "ratings_base = pd.read_csv('ml-100k/ub.base', sep='\\t', names=r_cols, encoding='latin-1')\n",
    "ratings_test = pd.read_csv('ml-100k/ub.test', sep='\\t', names=r_cols, encoding='latin-1')\n",
    "\n",
    "rate_train = ratings_base.values\n",
    "rate_test = ratings_test.values\n",
    "\n",
    "# indices start from 0\n",
    "rate_train[:, :2] -= 1\n",
    "rate_test[:, :2] -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f257b57-3ada-425d-9680-d92dbdccdec0",
   "metadata": {},
   "source": [
    "Công việc quan trọng trong **content-based recommendation system** là xây dựng **profile cho mỗi item**, tức feature vector cho mỗi item. Trước hết, chúng ta cần load toàn bộ thông tin về các items vào biến items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1df01fad-4720-4138-b4c2-15f1f29a88b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 10 , loss = 5.6520875576363645 , RMSE train = 1.2072871126063933\n",
      "iter = 20 , loss = 2.637805390579564 , RMSE train = 1.037690247600886\n",
      "iter = 30 , loss = 1.342683088243036 , RMSE train = 1.0294513946151267\n",
      "iter = 40 , loss = 0.7525635125008245 , RMSE train = 1.0291939508301553\n",
      "iter = 50 , loss = 0.4820965499009478 , RMSE train = 1.0292065868506366\n",
      "iter = 60 , loss = 0.3580667117574333 , RMSE train = 1.0292124752628884\n",
      "iter = 70 , loss = 0.30118735970103944 , RMSE train = 1.029213915077384\n",
      "iter = 80 , loss = 0.27510294107329125 , RMSE train = 1.0292142391077674\n",
      "iter = 90 , loss = 0.26314088638286587 , RMSE train = 1.0292143107839382\n",
      "iter = 100 , loss = 0.257655218851435 , RMSE train = 1.0292143265592737\n",
      "\n",
      "User-based MF, RMSE = 1.0603799033039885\n"
     ]
    }
   ],
   "source": [
    "rs = MF(rate_train, K = 10, lam = .1, print_every = 10, \n",
    "    learning_rate = 0.75, max_iter = 100, user_based = 1)\n",
    "rs.fit()\n",
    "# evaluate on test data\n",
    "RMSE = rs.evaluate_RMSE(rate_test)\n",
    "print ('\\nUser-based MF, RMSE =', RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f1e6ce-2888-4ebe-83a3-2a3090a22c4b",
   "metadata": {},
   "source": [
    "Ta nhận thấy rằng giá trị loss giảm dần và RMSE train cũng giảm dần khi số vòng lặp tăng lên. RMSE có cao hơn so với Neighborhood-based Collaborative Filtering (khoảng 0.99) một chút nhưng vẫn tốt hơn kết quả của Content-based Recommendation Systems rất nhiều (khoảng 1.2). Nếu chuẩn hoá dựa trên item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9df7e92-dfca-418a-9f55-922a4429a244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 10 , loss = 5.635751992551238 , RMSE train = 1.1824586781036408\n",
      "iter = 20 , loss = 2.620743564870659 , RMSE train = 1.0060797296427966\n",
      "iter = 30 , loss = 1.3255148798610152 , RMSE train = 0.9966774496847448\n",
      "iter = 40 , loss = 0.7354659481324013 , RMSE train = 0.9962134006066676\n",
      "iter = 50 , loss = 0.46506116090939464 , RMSE train = 0.9961842377101514\n",
      "iter = 60 , loss = 0.3410669313852371 , RMSE train = 0.9961813830267512\n",
      "iter = 70 , loss = 0.28420571763004254 , RMSE train = 0.9961809806897908\n",
      "iter = 80 , loss = 0.25813013854841055 , RMSE train = 0.9961809159850099\n",
      "iter = 90 , loss = 0.24617231184102212 , RMSE train = 0.9961809056025401\n",
      "iter = 100 , loss = 0.24068864957970462 , RMSE train = 0.9961809040547429\n",
      "\n",
      "Item-based MF, RMSE = 1.049804754072604\n"
     ]
    }
   ],
   "source": [
    "rs = MF(rate_train, K = 10, lam = .1, print_every = 10, learning_rate = 0.75, max_iter = 100, user_based = 0)\n",
    "rs.fit()\n",
    "# evaluate on test data\n",
    "RMSE = rs.evaluate_RMSE(rate_test)\n",
    "print ('\\nItem-based MF, RMSE =', RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e60e3-eaa5-4fa5-9fe1-e3ddc5114277",
   "metadata": {},
   "source": [
    "Kết quả có tốt hơn một chút. \r\n",
    "Chúng ta cùng làm thêm một thí nghiệm nữa khi không sử dụng regularization, tức lam = (Nếu các bạn chạy đoạn code trên, các bạn sẽ thấy chất lượng của mô hình giảm đi rõ rệt (RMSE cao).0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5fe3e55-dd4e-4daa-8cd9-c37590db6f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 10 , loss = 1.168460384526038 , RMSE train = 1.489457736855561\n",
      "iter = 20 , loss = 1.102163965548662 , RMSE train = 1.468744958197372\n",
      "iter = 30 , loss = 1.0432701086935585 , RMSE train = 1.4491940997243578\n",
      "iter = 40 , loss = 0.9906854924947858 , RMSE train = 1.4308341771954574\n",
      "iter = 50 , loss = 0.9435140006740721 , RMSE train = 1.413541905477885\n",
      "iter = 60 , loss = 0.9010152199332621 , RMSE train = 1.3971776462936742\n",
      "iter = 70 , loss = 0.8625728299743316 , RMSE train = 1.381771078211105\n",
      "iter = 80 , loss = 0.8276702707313887 , RMSE train = 1.3672944450824986\n",
      "iter = 90 , loss = 0.7958718266004444 , RMSE train = 1.3536282672325342\n",
      "iter = 100 , loss = 0.7668077872175593 , RMSE train = 1.3406934726912483\n",
      "\n",
      "Item-based MF, RMSE = 1.4167364263808038\n"
     ]
    }
   ],
   "source": [
    "rs = MF(rate_train, K = 2, lam = 0, print_every = 10, learning_rate = 1, max_iter = 100, user_based = 0)\n",
    "rs.fit()\n",
    "# evaluate on test data\n",
    "RMSE = rs.evaluate_RMSE(rate_test)\n",
    "print ('\\nItem-based MF, RMSE =', RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ea3305-fb10-4c16-88ae-755d58510b7d",
   "metadata": {},
   "source": [
    "**Áp dụng lên MovieLens 1M**\n",
    "\n",
    "Tiếp theo, chúng ta cùng đến với một bộ cơ sở dữ liệu lớn hơn là MovieLens 1M bao gồm xấp xỉ 1 triệu ratings của 6000 người dùng lên 4000 bộ phim. Đây là một bộ cơ sở dữ liệu lớn, thời gian training cũng sẽ tăng theo. Bạn đọc cũng có thể thử áp dụng mô hình Neighborhood-based Collaborative Filtering lên cơ sở dữ liệu này để so sánh kết quả. Dự đoán rằng thời gian training sẽ nhanh nhưng thời gian inference sẽ rất lâu.\n",
    "\n",
    "Load dữ liệu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d75a371e-7057-4b4f-8150-8725e19e242f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_4152\\2214174175.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  ratings_base = pd.read_csv('ml-1m/ratings.dat', sep='::', names=r_cols, encoding='latin-1')\n"
     ]
    }
   ],
   "source": [
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "\n",
    "ratings_base = pd.read_csv('ml-1m/ratings.dat', sep='::', names=r_cols, encoding='latin-1')\n",
    "ratings = ratings_base.values\n",
    "\n",
    "# indices in Python start from 0\n",
    "ratings[:, :2] -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d7bce3-3ed4-4a44-91f7-c21d54e6d21a",
   "metadata": {},
   "source": [
    "Tách tập training và test, sử dụng 1/3 dữ liệu cho test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7203353-1b46-4bb9-af98-ab43808553e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670140, 4) (330069, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rate_train, rate_test = train_test_split(ratings, test_size=0.33, random_state=42)\n",
    "print (rate_train.shape, rate_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2294d6-8118-470c-9e8d-deb12ee1266c",
   "metadata": {},
   "source": [
    "Áp dụng Matrix Factorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22620156-20b3-4569-b793-57a1fb777f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 2 , loss = 6.761747887511635 , RMSE train = 1.1143284735727872\n",
      "iter = 4 , loss = 4.327793358024362 , RMSE train = 1.0008171453888475\n",
      "iter = 6 , loss = 2.835854040659816 , RMSE train = 0.9779611211532075\n",
      "iter = 8 , loss = 1.8920648742108626 , RMSE train = 0.9740430986630146\n",
      "iter = 10 , loss = 1.2899052574960435 , RMSE train = 0.9733906310904621\n",
      "\n",
      "Item-based MF, RMSE = 0.9816312828393646\n"
     ]
    }
   ],
   "source": [
    "rs = MF(rate_train, K = 2, lam = 0.1, print_every = 2, learning_rate = 2, max_iter = 10, user_based = 0)\n",
    "rs.fit()\n",
    "# evaluate on test data\n",
    "RMSE = rs.evaluate_RMSE(rate_test)\n",
    "print ('\\nItem-based MF, RMSE =', RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc10095-532d-4d68-9284-86c5e7224363",
   "metadata": {},
   "source": [
    "Kết quả khá ấn tượng sau 10 vòng lặp. Kết quả khi áp dụng Neighborhood-based Collaborative Filtering là khoảng 0.92 nhưng thời gian inference khá lớn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ef8c8-5a40-4058-89b5-19897d957158",
   "metadata": {},
   "source": [
    "SV tham khảo thêm bài lab khác có cùng chủ đề tại đây:\n",
    "\n",
    "https://github.com/wangyuhsin/matrix-factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74930968-9d79-47e7-b51a-f55e5016d2ea",
   "metadata": {},
   "source": [
    "**Bài 3.** Thao tác content-based với CSDL **movie-len 100k** với cách khác. SV download tại đây (giải nén và đặt cùng địa chỉ file notebook):\n",
    "https://grouplens.org/datasets/movielens/100k/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd8c75-d066-44c5-9e80-e67d0c839875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "#Reading user file:\n",
    "u_cols =  ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
    "users = pd.read_csv('ml-100k/u.user', sep='|', names=u_cols,\n",
    " encoding='latin-1')\n",
    "\n",
    "n_users = users.shape[0]\n",
    "print ('Number of users:', n_users)\n",
    "# users.head() #uncomment this to see some few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca8cdb-16fb-49b7-908d-e4014045a247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading ratings file:\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "\n",
    "ratings_base = pd.read_csv('ml-100k/ua.base', sep='\\t', names=r_cols, encoding='latin-1')\n",
    "ratings_test = pd.read_csv('ml-100k/ua.test', sep='\\t', names=r_cols, encoding='latin-1')\n",
    "\n",
    "rate_train = ratings_base.values\n",
    "rate_test = ratings_test.values\n",
    "\n",
    "print ('Number of traing rates:', rate_train.shape[0])\n",
    "print ('Number of test rates:', rate_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36583c0-134a-43d8-9a72-1c5fd081c20e",
   "metadata": {},
   "source": [
    "Công việc quan trọng trong **content-based recommendation system** là xây dựng **profile cho mỗi item**, tức feature vector cho mỗi item. Trước hết, chúng ta cần load toàn bộ thông tin về các items vào biến items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3393313-5e80-44b2-867c-818f35e0c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading items file:\n",
    "i_cols = ['movie id', 'movie title' ,'release date','video release date', 'IMDb URL', 'unknown', 'Action', 'Adventure',\n",
    " 'Animation', 'Children\\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    " 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "items = pd.read_csv('ml-100k/u.item', sep='|', names=i_cols,\n",
    " encoding='latin-1')\n",
    "\n",
    "n_items = items.shape[0]\n",
    "print ('Number of items:', n_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0238b3c4-c3ce-4a1b-915d-283eed19cd9d",
   "metadata": {},
   "source": [
    "Vì ta đang dựa trên thể loại của phim để xây dựng profile, ta sẽ chỉ quan tâm tới 19 giá trị nhị phân ở cuối mỗi hàng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca6620-66d9-4493-bd02-8b613d2763d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = items.values\n",
    "X_train_counts = X0[:, -19:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6404ed25-575d-45ec-a78e-796e7a081f2a",
   "metadata": {},
   "source": [
    "Tiếp theo, chúng ta sẽ xây dựng feature vector cho mỗi item dựa trên ma trận thể loại phim và **feature TF-IDF**. Chúng ta sử dụng thư viện sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75efd133-63fc-47cc-9d20-38f99771db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=True, norm ='l2')\n",
    "tfidf = transformer.fit_transform(X_train_counts.tolist()).toarray()\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d31be49-339c-4b2c-bcbb-ecd0c94bd1e7",
   "metadata": {},
   "source": [
    "Sau bước này, mỗi hàng của tfidf tương ứng với feature vector của một bộ phim. Tiếp theo, với mỗi user, chúng ta cần đi tìm những bộ phim nào mà user đó đã rated, và giá trị của các rating đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d315b7a-bbe0-4077-848b-c0fddc790090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_items_rated_by_user(rate_matrix, user_id):\n",
    "    \"\"\"\n",
    "    in each line of rate_matrix, we have infor: user_id, item_id, rating (scores), time_stamp\n",
    "    we care about the first three values\n",
    "    return (item_ids, scores) rated by user user_id\n",
    "    \"\"\"\n",
    "    y = rate_matrix[:,0] # all users\n",
    "    # item indices rated by user_id\n",
    "    # we need to +1 to user_id since in the rate_matrix, id starts from 1 \n",
    "    # while index in python starts from 0\n",
    "    ids = np.where(y == user_id +1)[0] \n",
    "    item_ids = rate_matrix[ids, 1] - 1 # index starts from 0 \n",
    "    scores = rate_matrix[ids, 2]\n",
    "    return (item_ids, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d182cc9f-d658-48f3-90a0-76048a82c81b",
   "metadata": {},
   "source": [
    "Bây giờ, ta có thể đi tìm các hệ số của **Ridge Regression** cho mỗi user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96ad53-95f5-4eb1-bd13-f057def77e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "\n",
    "d = tfidf.shape[1] # data dimension\n",
    "W = np.zeros((d, n_users))\n",
    "b = np.zeros((1, n_users))\n",
    "\n",
    "for n in range(n_users):    \n",
    "    ids, scores = get_items_rated_by_user(rate_train, n)\n",
    "    clf = Ridge(alpha=0.01, fit_intercept  = True)\n",
    "    Xhat = tfidf[ids, :]\n",
    "    \n",
    "    clf.fit(Xhat, scores) \n",
    "    W[:, n] = clf.coef_\n",
    "    b[0, n] = clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f98a2cc-9821-4f25-a440-2be02c6a1858",
   "metadata": {},
   "source": [
    "Sau khi tính được các hệ số W và b, ratings cho mỗi items được dự đoán bằng cách tính:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f7309-081f-4f21-830d-45cf11593449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted scores\n",
    "Yhat = tfidf.dot(W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff595a7c-dfeb-40ec-b34e-7103ce4a286c",
   "metadata": {},
   "source": [
    "Dưới đây là một ví dụ với user có $id$ là 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d755784-f1fe-4c78-81bb-f75a4493e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "np.set_printoptions(precision=2) # 2 digits after . \n",
    "ids, scores = get_items_rated_by_user(rate_test, n)\n",
    "Yhat[n, ids]\n",
    "print('Rated movies ids :', ids )\n",
    "print('True ratings     :', scores)\n",
    "print('Predicted ratings:', Yhat[ids, n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0131ae-4166-4986-a961-689b5ade04a1",
   "metadata": {},
   "source": [
    "Để đánh giá mô hình tìm được, chúng ta sẽ sử dụng **Root Mean Squared Error (RMSE)**, tức căn bậc hai của trung bình cộng bình phương của lỗi. Lỗi được tính là hiệu của true rating và predicted rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70383834-119b-4ae7-9471-98410d56f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def evaluate(Yhat, rates, W, b):\n",
    "    se = 0\n",
    "    cnt = 0\n",
    "    for n in range(n_users):\n",
    "        ids, scores_truth = get_items_rated_by_user(rates, n)\n",
    "        scores_pred = Yhat[ids, n]\n",
    "        e = scores_truth - scores_pred \n",
    "        se += (e*e).sum(axis = 0)\n",
    "        cnt += e.size \n",
    "    return math.sqrt(se/cnt)\n",
    "\n",
    "print ('RMSE for training:', evaluate(Yhat, rate_train, W, b))\n",
    "print ('RMSE for test    :', evaluate(Yhat, rate_test, W, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5c0cb6-2798-48a4-a9c9-085b3b44e281",
   "metadata": {},
   "source": [
    "Như vậy, với tập training, sai số vào khoảng 0.9 sao; với tập test, sai số lớn hơn một chút, rơi vào khoảng 1.3. Chúng ta thấy rằng kết quả này chưa thực sự tốt vì chúng ta đã đơn giản hoá mô hình đi quá nhiều. Kết quả tốt hơn có thể được thấy khi dùng **Collaborative Filtering.**\n",
    "\n",
    "**Content-based Recommendation Systems** là phương pháp đơn giản nhất trong các hệ thống Recommendation Systems. Đặc điểm của phương pháp này là việc xây dựng mô hình cho mỗi user **không phụ thuộc vào các users khác**. \n",
    "Việc xây dựng mô hình cho mỗi users có thể được coi như bài toán Regression hoặc Classsification với training data là cặp dữ liệu (item profile, rating) mà user đó đã rated. item profile không phụ thuộc vào user, nó thường phụ thuộc vào các đặc điểm mô tả của item hoặc cũng có thể được xác định bằng cách yêu cầu người dùng gắn tag."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
